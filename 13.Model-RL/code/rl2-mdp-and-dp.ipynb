{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nnp.random.seed(0)\n# 定义状态转移概率矩阵P\nP = [\n    [0.9, 0.1, 0.0, 0.0, 0.0, 0.0],\n    [0.5, 0.0, 0.5, 0.0, 0.0, 0.0],\n    [0.0, 0.0, 0.0, 0.6, 0.0, 0.4],\n    [0.0, 0.0, 0.0, 0.0, 0.3, 0.7],\n    [0.0, 0.2, 0.3, 0.5, 0.0, 0.0],\n    [0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n]\nP = np.array(P)\n\nrewards = [-1, -2, -2, 10, 1, 0]  # 定义奖励函数\ngamma = 0.5  # 定义折扣因子","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-02T06:45:57.032996Z","iopub.execute_input":"2024-04-02T06:45:57.034079Z","iopub.status.idle":"2024-04-02T06:45:57.079268Z","shell.execute_reply.started":"2024-04-02T06:45:57.034034Z","shell.execute_reply":"2024-04-02T06:45:57.077726Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def cal_rewards(st,path,gamma):\n    res=0\n    for i in reversed(range(st,len(path))):\n        res=gamma*res+rewards[path[i]-1]\n    return res\npath=[1,2,3,6]\nst=0\ncal_rewards(0,path,0.5)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T06:51:32.444115Z","iopub.execute_input":"2024-04-02T06:51:32.444557Z","iopub.status.idle":"2024-04-02T06:51:32.455872Z","shell.execute_reply.started":"2024-04-02T06:51:32.444525Z","shell.execute_reply":"2024-04-02T06:51:32.454319Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"-2.5"},"metadata":{}}]},{"cell_type":"code","source":"\"\"\"\nP:状态转移概率矩阵,(n,n)\nR:奖励矩阵，（n,1）\ngamma:折扣因子\nn:状态数量\n公式：V=(I-gamma*P)^(-1)*R\n\"\"\"\n#得到各个状态价值\ndef get_stateVal(P,R,gamma,n):\n    R=np.array(R).reshape((-1,1))\n    value=np.dot(np.linalg.inv(np.eye(n,n)-gamma*P),R)\n    return value\nV=get_stateVal(P,rewards,gamma,6)\nV","metadata":{"execution":{"iopub.status.busy":"2024-04-02T07:00:10.726317Z","iopub.execute_input":"2024-04-02T07:00:10.726938Z","iopub.status.idle":"2024-04-02T07:00:10.753904Z","shell.execute_reply.started":"2024-04-02T07:00:10.726889Z","shell.execute_reply":"2024-04-02T07:00:10.751918Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"array([[-2.01950168],\n       [-2.21451846],\n       [ 1.16142785],\n       [10.53809283],\n       [ 3.58728554],\n       [ 0.        ]])"},"metadata":{}}]},{"cell_type":"code","source":"S = [\"s1\", \"s2\", \"s3\", \"s4\", \"s5\"]  # 状态集合\nA = [\"保持s1\", \"前往s1\", \"前往s2\", \"前往s3\", \"前往s4\", \"前往s5\", \"概率前往\"]  # 动作集合\n# 状态转移函数\nP = {\n    \"s1-保持s1-s1\": 1.0,\n    \"s1-前往s2-s2\": 1.0,\n    \"s2-前往s1-s1\": 1.0,\n    \"s2-前往s3-s3\": 1.0,\n    \"s3-前往s4-s4\": 1.0,\n    \"s3-前往s5-s5\": 1.0,\n    \"s4-前往s5-s5\": 1.0,\n    \"s4-概率前往-s2\": 0.2,\n    \"s4-概率前往-s3\": 0.4,\n    \"s4-概率前往-s4\": 0.4,\n}\n# 奖励函数\nR = {\n    \"s1-保持s1\": -1,\n    \"s1-前往s2\": 0,\n    \"s2-前往s1\": -1,\n    \"s2-前往s3\": -2,\n    \"s3-前往s4\": -2,\n    \"s3-前往s5\": 0,\n    \"s4-前往s5\": 10,\n    \"s4-概率前往\": 1,\n}\ngamma = 0.5  # 折扣因子\nMDP = (S, A, P, R, gamma)\n\n# 策略1,随机策略\nPi_1 = {\n    \"s1-保持s1\": 0.5,\n    \"s1-前往s2\": 0.5,\n    \"s2-前往s1\": 0.5,\n    \"s2-前往s3\": 0.5,\n    \"s3-前往s4\": 0.5,\n    \"s3-前往s5\": 0.5,\n    \"s4-前往s5\": 0.5,\n    \"s4-概率前往\": 0.5,\n}\n# 策略2\nPi_2 = {\n    \"s1-保持s1\": 0.6,\n    \"s1-前往s2\": 0.4,\n    \"s2-前往s1\": 0.3,\n    \"s2-前往s3\": 0.7,\n    \"s3-前往s4\": 0.5,\n    \"s3-前往s5\": 0.5,\n    \"s4-前往s5\": 0.1,\n    \"s4-概率前往\": 0.9,\n}\n\n\n# 把输入的两个字符串通过“-”连接,便于使用上述定义的P、R变量\ndef join(str1, str2):\n    return str1 + '-' + str2","metadata":{"execution":{"iopub.status.busy":"2024-04-02T07:21:55.315037Z","iopub.execute_input":"2024-04-02T07:21:55.315529Z","iopub.status.idle":"2024-04-02T07:21:55.330430Z","shell.execute_reply.started":"2024-04-02T07:21:55.315493Z","shell.execute_reply":"2024-04-02T07:21:55.328898Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"gamma = 0.5\n# 转化后的MRP的状态转移矩阵\nP_from_mdp_to_mrp = [\n    [0.5, 0.5, 0.0, 0.0, 0.0],\n    [0.5, 0.0, 0.5, 0.0, 0.0],\n    [0.0, 0.0, 0.0, 0.5, 0.5],\n    [0.0, 0.1, 0.2, 0.2, 0.5],\n    [0.0, 0.0, 0.0, 0.0, 1.0],\n]\nP_from_mdp_to_mrp = np.array(P_from_mdp_to_mrp)\nR_from_mdp_to_mrp = [-0.5, -1.5, -1.0, 5.5, 0]\n\nV = get_stateVal(P_from_mdp_to_mrp, R_from_mdp_to_mrp, gamma, 5)\nprint(\"MDP中每个状态价值分别为\\n\", V)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T07:23:13.310912Z","iopub.execute_input":"2024-04-02T07:23:13.311461Z","iopub.status.idle":"2024-04-02T07:23:13.332239Z","shell.execute_reply.started":"2024-04-02T07:23:13.311419Z","shell.execute_reply":"2024-04-02T07:23:13.330636Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"MDP中每个状态价值分别为\n [[-1.22555411]\n [-1.67666232]\n [ 0.51890482]\n [ 6.0756193 ]\n [ 0.        ]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 蒙特卡洛采样","metadata":{}},{"cell_type":"code","source":"def sample(MDP, Pi, timestep_max, number):\n    ''' 采样函数,策略Pi,限制最长时间步timestep_max,总共采样序列数number '''\n    S, A, P, R, gamma = MDP\n    paths=[]\n    for _ in range(number):\n        path=[]\n        t=0\n        now=S[np.random.randint(0,len(S)-1)]\n        while now!=S[-1] and t<timestep_max:\n            t+=1\n            rd,tp=np.random.rand(),0\n            for opt in A:\n                tp+=Pi.get(join(now,opt),0)\n                if tp>rd:\n                    a=opt\n                    r=R.get(join(now,opt),0)\n                    break\n            rd,tp=np.random.rand(),0\n            for s in S:\n                tp+=P.get(join(join(now,a),s),0)\n                if tp>rd:\n                    \n                    nxt=s\n                    break\n            path.append((now,a,r,nxt))\n            now=nxt\n        paths.append(path)\n    return paths\n            \n# 采样5次,每个序列最长不超过20步\npaths = sample(MDP, Pi_1, 20, 5)\nprint('第一条序列\\n', paths[0])\nprint('第二条序列\\n', paths[1])\nprint('第五条序列\\n', paths[4])         \n        ","metadata":{"execution":{"iopub.status.busy":"2024-04-02T07:42:47.656409Z","iopub.execute_input":"2024-04-02T07:42:47.656788Z","iopub.status.idle":"2024-04-02T07:42:47.673653Z","shell.execute_reply.started":"2024-04-02T07:42:47.656759Z","shell.execute_reply":"2024-04-02T07:42:47.672110Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"第一条序列\n [('s1', '前往s2', 0, 's2'), ('s2', '前往s3', -2, 's3'), ('s3', '前往s5', 0, 's5')]\n第二条序列\n [('s2', '前往s1', -1, 's1'), ('s1', '前往s2', 0, 's2'), ('s2', '前往s3', -2, 's3'), ('s3', '前往s4', -2, 's4'), ('s4', '概率前往', 1, 's4'), ('s4', '前往s5', 10, 's5')]\n第五条序列\n [('s1', '前往s2', 0, 's2'), ('s2', '前往s1', -1, 's1'), ('s1', '保持s1', -1, 's1'), ('s1', '保持s1', -1, 's1'), ('s1', '保持s1', -1, 's1'), ('s1', '前往s2', 0, 's2'), ('s2', '前往s3', -2, 's3'), ('s3', '前往s5', 0, 's5')]\n","output_type":"stream"}]},{"cell_type":"code","source":"def MC(paths,V,N,gamma):\n    for path in paths:\n        res=0\n        for i in range(len(path)-1,-1,-1):\n            (s,a,r,s_nxt)=path[i]\n            res=gamma*res+r\n            N[s]+=1\n            V[s]+=(res-V[s])/N[s]\n            \n            \ntimestep_max = 20\n# 采样1000次,可以自行修改\npaths= sample(MDP, Pi_1, timestep_max, 1000)\ngamma = 0.5\nV = {\"s1\": 0, \"s2\": 0, \"s3\": 0, \"s4\": 0, \"s5\": 0}\nN = {\"s1\": 0, \"s2\": 0, \"s3\": 0, \"s4\": 0, \"s5\": 0}\nMC(paths, V, N, gamma)\nprint(\"使用蒙特卡洛方法计算MDP的状态价值为\\n\", V)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T07:47:52.012058Z","iopub.execute_input":"2024-04-02T07:47:52.012512Z","iopub.status.idle":"2024-04-02T07:47:52.085480Z","shell.execute_reply.started":"2024-04-02T07:47:52.012482Z","shell.execute_reply":"2024-04-02T07:47:52.083917Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"使用蒙特卡洛方法计算MDP的状态价值为\n {'s1': -1.2328340845543146, 's2': -1.6968010685197825, 's3': 0.492886573447705, 's4': 6.018865054941437, 's5': 0}\n","output_type":"stream"}]},{"cell_type":"code","source":"def occupancy(episodes, s, a, timestep_max, gamma):\n    ''' 计算状态动作对（s,a）出现的频率,以此来估算策略的占用度量 '''\n    rho = 0\n    total_times = np.zeros(timestep_max)  # 记录每个时间步t各被经历过几次\n    occur_times = np.zeros(timestep_max)  # 记录(s_t,a_t)=(s,a)的次数\n    for episode in episodes:\n        for i in range(len(episode)):\n            (s_opt, a_opt, r, s_next) = episode[i]\n            total_times[i] += 1\n            if s == s_opt and a == a_opt:\n                occur_times[i] += 1\n    for i in reversed(range(timestep_max)):\n        if total_times[i]:\n            rho += gamma**i * occur_times[i] / total_times[i]\n    return (1 - gamma) * rho\n\n\ngamma = 0.5\ntimestep_max = 1000\n\nepisodes_1 = sample(MDP, Pi_1, timestep_max, 1000)\nepisodes_2 = sample(MDP, Pi_2, timestep_max, 1000)\nrho_1 = occupancy(episodes_1, \"s4\", \"概率前往\", timestep_max, gamma)\nrho_2 = occupancy(episodes_2, \"s4\", \"概率前往\", timestep_max, gamma)\nprint(rho_1, rho_2)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T07:51:04.153738Z","iopub.execute_input":"2024-04-02T07:51:04.154206Z","iopub.status.idle":"2024-04-02T07:51:04.387393Z","shell.execute_reply.started":"2024-04-02T07:51:04.154175Z","shell.execute_reply":"2024-04-02T07:51:04.385875Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"0.11230466834067263 0.23299446031885906\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 悬崖漫步环境","metadata":{}},{"cell_type":"code","source":"import copy\n\n\nclass CliffWalkingEnv:\n    \"\"\" 悬崖漫步环境\"\"\"\n    def __init__(self, ncol=12, nrow=4):\n        self.ncol = ncol  # 定义网格世界的列\n        self.nrow = nrow  # 定义网格世界的行\n        # 转移矩阵P[state][action] = [(p, next_state, reward, done)]包含下一个状态和奖励\n        self.P = self.createP()\n\n    def createP(self):\n        # 初始化\n        P = [[[] for j in range(4)] for i in range(self.nrow * self.ncol)]\n        # 4种动作, change[0]:上,change[1]:下, change[2]:左, change[3]:右。坐标系原点(0,0)\n        # 定义在左上角\n        change = [[0, -1], [0, 1], [-1, 0], [1, 0]]\n        for i in range(self.nrow):\n            for j in range(self.ncol):\n                for a in range(4):\n                    # 位置在悬崖或者目标状态,因为无法继续交互,任何动作奖励都为0\n                    if i == self.nrow - 1 and j > 0:\n                        P[i * self.ncol + j][a] = [(1, i * self.ncol + j, 0,\n                                                    True)]\n                        continue\n                    # 其他位置\n                    next_x = min(self.ncol - 1, max(0, j + change[a][0]))\n                    next_y = min(self.nrow - 1, max(0, i + change[a][1]))\n                    next_state = next_y * self.ncol + next_x\n                    reward = -1\n                    done = False\n                    # 下一个位置在悬崖或者终点\n                    if next_y == self.nrow - 1 and next_x > 0:\n                        done = True\n                        if next_x != self.ncol - 1:  # 下一个位置在悬崖\n                            reward = -100\n                    P[i * self.ncol + j][a] = [(1, next_state, reward, done)]\n        return P","metadata":{"execution":{"iopub.status.busy":"2024-04-02T08:35:21.746423Z","iopub.execute_input":"2024-04-02T08:35:21.746882Z","iopub.status.idle":"2024-04-02T08:35:21.804742Z","shell.execute_reply.started":"2024-04-02T08:35:21.746846Z","shell.execute_reply":"2024-04-02T08:35:21.803374Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"\"\"\"策略迭代\"\"\"\nclass policy_iter():\n    def __init__(self,env,theta,gamma):\n        self.env=env\n        self.theta=theta\n        self.gamma=gamma\n        self.v=[0]*self.env.ncol*self.env.nrow\n        self.pi=[\n            [0.25,0.25,0.25,0.25] for i in range(self.env.ncol*self.env.nrow)\n        ]\n    \n    ## 更新v\n    def policy_eval(self):\n        cnt=1\n        \n        while True:\n            max_diff=0\n            new_v=[0]*self.env.ncol*self.env.nrow\n            for s in range(self.env.ncol*self.env.nrow):\n                qsa_list=[]\n                for a in range(4):\n                    qsa=0\n                    for res in self.env.P[s][a]:\n                        p,new_state,r,done=res\n                        qsa+=p*(r+self.gamma*self.v[new_state]*(1-done))\n                    qsa_list.append(qsa*self.pi[s][a])\n                new_v[s]=sum(qsa_list)\n                max_diff=max(max_diff,abs(new_v[s]-self.v[s]))\n            self.v=new_v\n            if max_diff<self.theta:\n                break\n            cnt+=1\n        print('策略评估进行%d轮后完成'%cnt)\n            \n    ## 更新pi\n    def policy_improve(self):\n        for s in range(self.env.ncol*self.env.nrow):\n            qsa_list=[]\n            for a in range(4):\n                qsa=0\n                for res in self.env.P[s][a]:\n                    p,new_state,r,done=res\n                    qsa+=p*(r+self.gamma*self.v[new_state]*(1-done))\n                qsa_list.append(qsa)\n            maxq=max(qsa_list)\n            cntq=qsa_list.count(maxq)\n            \n            self.pi[s]=[1.0/cntq if q==maxq else 0 for q in qsa_list]\n        print('策略提升完成！')\n        return self.pi\n    def policy_iteration(self):\n        while(1):\n            self.policy_eval()\n            old_pi=copy.deepcopy(self.pi)\n            new_pi=self.policy_improve()\n            if old_pi==new_pi:\n                break\n                \n                    \n    ","metadata":{"execution":{"iopub.status.busy":"2024-04-02T08:39:05.493653Z","iopub.execute_input":"2024-04-02T08:39:05.494150Z","iopub.status.idle":"2024-04-02T08:39:05.512841Z","shell.execute_reply.started":"2024-04-02T08:39:05.494113Z","shell.execute_reply":"2024-04-02T08:39:05.511313Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def print_agent(agent, action_meaning, disaster=[], end=[]):\n    print(\"状态价值：\")\n    for i in range(agent.env.nrow):\n        for j in range(agent.env.ncol):\n            # 为了输出美观,保持输出6个字符\n            print('%6.6s' % ('%.3f' % agent.v[i * agent.env.ncol + j]), end=' ')\n        print()\n\n    print(\"策略：\")\n    for i in range(agent.env.nrow):\n        for j in range(agent.env.ncol):\n            # 一些特殊的状态,例如悬崖漫步中的悬崖\n            if (i * agent.env.ncol + j) in disaster:\n                print('****', end=' ')\n            elif (i * agent.env.ncol + j) in end:  # 目标状态\n                print('EEEE', end=' ')\n            else:\n                a = agent.pi[i * agent.env.ncol + j]\n                pi_str = ''\n                for k in range(len(action_meaning)):\n                    pi_str += action_meaning[k] if a[k] > 0 else 'o'\n                print(pi_str, end=' ')\n        print()\n\n\nenv = CliffWalkingEnv()\naction_meaning = ['^', 'v', '<', '>']\ntheta = 0.001\ngamma = 0.9\nagent = policy_iter(env, theta, gamma)\nagent.policy_iteration()\nprint_agent(agent, action_meaning, list(range(37, 47)), [47])","metadata":{"execution":{"iopub.status.busy":"2024-04-02T08:39:08.441670Z","iopub.execute_input":"2024-04-02T08:39:08.442120Z","iopub.status.idle":"2024-04-02T08:39:08.519688Z","shell.execute_reply.started":"2024-04-02T08:39:08.442088Z","shell.execute_reply":"2024-04-02T08:39:08.518558Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"策略评估进行60轮后完成\n策略提升完成！\n策略评估进行72轮后完成\n策略提升完成！\n策略评估进行44轮后完成\n策略提升完成！\n策略评估进行12轮后完成\n策略提升完成！\n策略评估进行1轮后完成\n策略提升完成！\n状态价值：\n-7.712 -7.458 -7.176 -6.862 -6.513 -6.126 -5.695 -5.217 -4.686 -4.095 -3.439 -2.710 \n-7.458 -7.176 -6.862 -6.513 -6.126 -5.695 -5.217 -4.686 -4.095 -3.439 -2.710 -1.900 \n-7.176 -6.862 -6.513 -6.126 -5.695 -5.217 -4.686 -4.095 -3.439 -2.710 -1.900 -1.000 \n-7.458  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 \n策略：\novo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovoo \novo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovoo \nooo> ooo> ooo> ooo> ooo> ooo> ooo> ooo> ooo> ooo> ooo> ovoo \n^ooo **** **** **** **** **** **** **** **** **** **** EEEE \n","output_type":"stream"}]},{"cell_type":"code","source":"\"\"\"策略迭代\"\"\"\nclass value_iter():\n    def __init__(self,env,theta,gamma):\n        self.env=env\n        self.theta=theta\n        self.gamma=gamma\n        self.v=[0]*self.env.ncol*self.env.nrow\n        self.pi=[\n            None for i in range(self.env.ncol*self.env.nrow)\n        ]\n    \n    ## 更新v\n    def value_iteration(self):\n        cnt=1\n        while True:\n            max_diff=0\n            new_v=[0]*self.env.ncol*self.env.nrow\n            for s in range(self.env.ncol*self.env.nrow):\n                qsa_list=[]\n                for a in range(4):\n                    qsa=0\n                    for res in self.env.P[s][a]:\n                        p,new_state,r,done=res\n                        qsa+=p*(r+self.gamma*self.v[new_state]*(1-done))\n                    qsa_list.append(qsa)\n                new_v[s]=max(qsa_list)\n                max_diff=max(max_diff,abs(new_v[s]-self.v[s]))\n            self.v=new_v\n            if max_diff<self.theta:\n                break\n            cnt+=1\n        print('价值迭代进行%d轮后完成'%cnt)\n        self.get_policy()\n            \n    ## 更新pi\n    def get_policy(self):\n        for s in range(self.env.ncol*self.env.nrow):\n            qsa_list=[]\n            for a in range(4):\n                qsa=0\n                for res in self.env.P[s][a]:\n                    p,new_state,r,done=res\n                    qsa+=p*(r+self.gamma*self.v[new_state]*(1-done))\n                qsa_list.append(qsa)\n            maxq=max(qsa_list)\n            cntq=qsa_list.count(maxq)\n            \n            self.pi[s]=[1.0/cntq if q==maxq else 0 for q in qsa_list]\n\n            \nenv = CliffWalkingEnv()\naction_meaning = ['^', 'v', '<', '>']\ntheta = 0.001\ngamma = 0.9\nagent = value_iter(env, theta, gamma)\nagent.value_iteration()\nprint_agent(agent, action_meaning, list(range(37, 47)), [47])","metadata":{"execution":{"iopub.status.busy":"2024-04-02T08:47:05.298844Z","iopub.execute_input":"2024-04-02T08:47:05.299362Z","iopub.status.idle":"2024-04-02T08:47:05.326108Z","shell.execute_reply.started":"2024-04-02T08:47:05.299327Z","shell.execute_reply":"2024-04-02T08:47:05.324741Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"价值迭代进行15轮后完成\n状态价值：\n-7.712 -7.458 -7.176 -6.862 -6.513 -6.126 -5.695 -5.217 -4.686 -4.095 -3.439 -2.710 \n-7.458 -7.176 -6.862 -6.513 -6.126 -5.695 -5.217 -4.686 -4.095 -3.439 -2.710 -1.900 \n-7.176 -6.862 -6.513 -6.126 -5.695 -5.217 -4.686 -4.095 -3.439 -2.710 -1.900 -1.000 \n-7.458  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 \n策略：\novo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovoo \novo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovoo \nooo> ooo> ooo> ooo> ooo> ooo> ooo> ooo> ooo> ooo> ooo> ovoo \n^ooo **** **** **** **** **** **** **** **** **** **** EEEE \n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 冰湖","metadata":{}},{"cell_type":"code","source":"import gym\nenv = gym.make(\"FrozenLake-v1\")  # 创建环境\nenv = env.unwrapped  # 解封装才能访问状态转移矩阵P\nenv.render()  # 环境渲染,通常是弹窗显示或打印出可视化的环境\n\nholes = set()\nends = set()\nfor s in env.P:\n    for a in env.P[s]:\n        for s_ in env.P[s][a]:\n            if s_[2] == 1.0:  # 获得奖励为1,代表是目标\n                ends.add(s_[1])\n            if s_[3] == True:\n                holes.add(s_[1])\nholes = holes - ends\nprint(\"冰洞的索引:\", holes)\nprint(\"目标的索引:\", ends)\n\nfor a in env.P[14]:  # 查看目标左边一格的状态转移信息\n    print(env.P[14][a])","metadata":{"execution":{"iopub.status.busy":"2024-04-02T08:56:12.922319Z","iopub.execute_input":"2024-04-02T08:56:12.922750Z","iopub.status.idle":"2024-04-02T08:56:12.935292Z","shell.execute_reply.started":"2024-04-02T08:56:12.922720Z","shell.execute_reply":"2024-04-02T08:56:12.933815Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"冰洞的索引: {11, 12, 5, 7}\n目标的索引: {15}\n[(0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False)]\n[(0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 15, 1.0, True)]\n[(0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 15, 1.0, True), (0.3333333333333333, 10, 0.0, False)]\n[(0.3333333333333333, 15, 1.0, True), (0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 13, 0.0, False)]\n","output_type":"stream"}]},{"cell_type":"code","source":"# 这个动作意义是Gym库针对冰湖环境事先规定好的\naction_meaning = ['<', 'v', '>', '^']\ntheta = 1e-5\ngamma = 0.9\nagent = policy_iter(env, theta, gamma)\nagent.policy_iteration()\nprint_agent(agent, action_meaning, [5, 7, 11, 12], [15])","metadata":{"execution":{"iopub.status.busy":"2024-04-02T08:51:10.246859Z","iopub.execute_input":"2024-04-02T08:51:10.247589Z","iopub.status.idle":"2024-04-02T08:51:10.270694Z","shell.execute_reply.started":"2024-04-02T08:51:10.247554Z","shell.execute_reply":"2024-04-02T08:51:10.269234Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"策略评估进行25轮后完成\n策略提升完成！\n策略评估进行58轮后完成\n策略提升完成！\n状态价值：\n 0.069  0.061  0.074  0.056 \n 0.092  0.000  0.112  0.000 \n 0.145  0.247  0.300  0.000 \n 0.000  0.380  0.639  0.000 \n策略：\n<ooo ooo^ <ooo ooo^ \n<ooo **** <o>o **** \nooo^ ovoo <ooo **** \n**** oo>o ovoo EEEE \n","output_type":"stream"}]},{"cell_type":"code","source":"action_meaning = ['<', 'v', '>', '^']\ntheta = 1e-5\ngamma = 0.9\nagent = value_iter(env, theta, gamma)\nagent.value_iteration()\nprint_agent(agent, action_meaning, [5, 7, 11, 12], [15])","metadata":{"execution":{"iopub.status.busy":"2024-04-02T08:52:27.393902Z","iopub.execute_input":"2024-04-02T08:52:27.394380Z","iopub.status.idle":"2024-04-02T08:52:27.411364Z","shell.execute_reply.started":"2024-04-02T08:52:27.394346Z","shell.execute_reply":"2024-04-02T08:52:27.410134Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"价值迭代进行61轮后完成\n状态价值：\n 0.069  0.061  0.074  0.056 \n 0.092  0.000  0.112  0.000 \n 0.145  0.247  0.300  0.000 \n 0.000  0.380  0.639  0.000 \n策略：\n<ooo ooo^ <ooo ooo^ \n<ooo **** <o>o **** \nooo^ ovoo <ooo **** \n**** oo>o ovoo EEEE \n","output_type":"stream"}]}]}